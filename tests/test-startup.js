#!/usr/bin/env node

/**
 * Test script to verify the automatic startup functionality
 * This script tests the configuration system and startup behavior
 */

import { readFileSync } from 'fs';
import { join } from 'path';

console.log('üß™ Testing Automatic LLM Startup Configuration...\n');

// Test 1: Verify default configuration is updated
console.log('1. Testing default configuration...');
try {
  const llmRsContent = readFileSync('src-tauri/src/llm.rs', 'utf8');
  const llmTsContent = readFileSync('src/types/llm.ts', 'utf8');

  // Check Rust configuration
  if (llmRsContent.includes('Llama-3.2-1B-Instruct-Q5_K_M.gguf')) {
    console.log('   ‚úÖ Rust default config updated to Llama model');
  } else {
    console.log('   ‚ùå Rust default config not updated');
  }

  if (llmRsContent.includes('Llama-3.2-1B-Instruct')) {
    console.log('   ‚úÖ Rust model name updated to Llama');
  } else {
    console.log('   ‚ùå Rust model name not updated');
  }

  // Check TypeScript configuration
  if (llmTsContent.includes('Llama-3.2-1B-Instruct-Q5_K_M.gguf')) {
    console.log('   ‚úÖ TypeScript default config updated to Llama model');
  } else {
    console.log('   ‚ùå TypeScript default config not updated');
  }

  if (llmTsContent.includes('Llama-3.2-1B-Instruct')) {
    console.log('   ‚úÖ TypeScript model name updated to Llama');
  } else {
    console.log('   ‚ùå TypeScript model name not updated');
  }

} catch (error) {
  console.log('   ‚ùå Error reading configuration files:', error.message);
}

// Test 2: Verify LLM context and provider setup
console.log('\n2. Testing LLM context and provider setup...');
try {
  const appContent = readFileSync('src/App.tsx', 'utf8');
  const contextContent = readFileSync('src/contexts/LlmContext.tsx', 'utf8');

  if (appContent.includes('LlmProvider')) {
    console.log('   ‚úÖ LlmProvider imported and used in App.tsx');
  } else {
    console.log('   ‚ùå LlmProvider not properly integrated');
  }

  if (contextContent.includes('autoInitializeAndStart')) {
    console.log('   ‚úÖ Auto-initialization function available in context');
  } else {
    console.log('   ‚ùå Auto-initialization function missing');
  }

} catch (error) {
  console.log('   ‚ùå Error reading context files:', error.message);
}

// Test 3: Verify configuration management system
console.log('\n3. Testing configuration management system...');
try {
  const configContent = readFileSync('src/config/app.ts', 'utf8');

  if (configContent.includes('AppConfigManager')) {
    console.log('   ‚úÖ AppConfigManager class implemented');
  } else {
    console.log('   ‚ùå AppConfigManager class missing');
  }

  if (configContent.includes('autoStartLlm: true')) {
    console.log('   ‚úÖ Auto-start enabled by default');
  } else {
    console.log('   ‚ùå Auto-start not enabled by default');
  }

  if (configContent.includes('resolveModelPath')) {
    console.log('   ‚úÖ Model path resolution function implemented');
  } else {
    console.log('   ‚ùå Model path resolution function missing');
  }

} catch (error) {
  console.log('   ‚ùå Error reading configuration files:', error.message);
}

// Test 4: Verify useLlm hook auto-initialization
console.log('\n4. Testing useLlm hook auto-initialization...');
try {
  const hookContent = readFileSync('src/hooks/useLlm.ts', 'utf8');

  if (hookContent.includes('autoInitializeAndStart')) {
    console.log('   ‚úÖ Auto-initialization function implemented');
  } else {
    console.log('   ‚ùå Auto-initialization function missing');
  }

  if (hookContent.includes('AppConfigManager.loadConfig()')) {
    console.log('   ‚úÖ Configuration loading integrated');
  } else {
    console.log('   ‚ùå Configuration loading not integrated');
  }

  if (hookContent.includes('retryAttempts')) {
    console.log('   ‚úÖ Retry mechanism implemented');
  } else {
    console.log('   ‚ùå Retry mechanism missing');
  }

} catch (error) {
  console.log('   ‚ùå Error reading hook files:', error.message);
}

// Test 5: Verify settings component
console.log('\n5. Testing settings component...');
try {
  const settingsContent = readFileSync('src/components/AppSettings.tsx', 'utf8');
  const appContent = readFileSync('src/App.tsx', 'utf8');

  if (settingsContent.includes('AppConfigManager')) {
    console.log('   ‚úÖ Settings component uses AppConfigManager');
  } else {
    console.log('   ‚ùå Settings component missing AppConfigManager integration');
  }

  if (appContent.includes('AppSettings')) {
    console.log('   ‚úÖ Settings component added to app tabs');
  } else {
    console.log('   ‚ùå Settings component not added to app tabs');
  }

} catch (error) {
  console.log('   ‚ùå Error reading settings files:', error.message);
}

// Test 6: Check model directory and file
console.log('\n6. Testing model directory and file...');
try {
  const { statSync } = await import('fs');

  try {
    const modelDir = statSync('models');
    if (modelDir.isDirectory()) {
      console.log('   ‚úÖ Models directory created');
    } else {
      console.log('   ‚ùå Models directory not found');
    }
  } catch {
    console.log('   ‚ùå Models directory not found');
  }

  try {
    const modelFile = statSync('models/Llama-3.2-1B-Instruct-Q5_K_M.gguf');
    if (modelFile.isFile()) {
      const sizeMB = (modelFile.size / (1024 * 1024)).toFixed(2);
      console.log(`   ‚úÖ Llama model file exists (${sizeMB} MB)`);

      // Llama 3.2 1B model is typically around 800-900 MB for Q5_K_M
      if (modelFile.size > 100 * 1024 * 1024) {
        console.log('   ‚úÖ Llama model file appears to be complete');
      } else {
        console.log('   ‚è≥ Llama model file seems incomplete...');
      }
    } else {
      console.log('   ‚ùå Llama model file not found');
    }
  } catch {
    console.log('   ‚ùå Llama model file not found');
  }

} catch (error) {
  console.log('   ‚ùå Error checking model files:', error.message);
}

console.log('\nüéØ Test Summary:');
console.log('   - Default configurations updated to use Llama-3.2-1B-Instruct model');
console.log('   - LLM context and provider system implemented');
console.log('   - Configuration management system created');
console.log('   - Auto-initialization with retry mechanism added');
console.log('   - Settings component for user configuration');
console.log('   - Llama model ready for use');

console.log('\nüìã Next Steps:');
console.log('   1. Test the application startup with Llama model');
console.log('   2. Verify auto-initialization works');
console.log('   3. Test manual configuration through settings');
console.log('   4. Verify chat functionality with Llama model');
console.log('   5. Test model performance and response quality');

console.log('\n‚ú® Configuration Updated! The Tauri application now:');
console.log('   ‚Ä¢ Uses Llama-3.2-1B-Instruct model for AI chat');
console.log('   ‚Ä¢ Starts the LLM service automatically on app launch');
console.log('   ‚Ä¢ Provides seamless user experience without manual setup');
console.log('   ‚Ä¢ Includes retry mechanisms and error handling');
console.log('   ‚Ä¢ Offers user-configurable settings for advanced users');
